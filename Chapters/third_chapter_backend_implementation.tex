\section{ГЛАВА 6 РЕАЛИЗАЦИЯ И ТЕСТИРОВАНИЕ СЕРВЕРНОЙ ЧАСТИ}

\subsection*{6.1. Обзор технологий и инструментов}
\addcontentsline{toc}{subsection}{6.1. Обзор технологий и инструментов}

\subsubsection*{Golang}

Go 1.24.0 выбран в качестве основного языка разработки для всех микросервисов системы. Это решение обеспечивает простоту сопровождения: Go имеет лаконичный и выразительный синтаксис, строгую статическую типизацию и стандартные инструменты форматирования (например, gofmt), что упрощает чтение и поддержку кода. Кроме того, Go предоставляет эффективное управление памятью и масштабируемую модель конкурентности. Легковесные горутины и оптимизированный сборщик мусора позволяют сервисам обрабатывать множество одновременных соединений с низкими накладными расходами. Также стоит отметить достоинства экосистемы и инструментария.

\begin{itemize}
    \item Детерминированность сборки: Механизм модулей Go (go.mod и go.sum) строго фиксирует версии всех зависимостей и их контрольные суммы. Это гарантирует, что сборки на локальных машинах, в CI/CD и на продакшн-среде будут идентичными, исключая «эффект снежинки». Благодаря этому на каждом этапе конвейера мы получаем точно известный набор библиотек, что повышает воспроизводимость сборок и избавляет от неожиданных регрессий.
    \item Контроль качества кода: для поддержки единообразия стиля и обнаружения ошибок применяется статический анализатор golangci-lint. Этот инструмент запускает множество линтеров параллельно (проверка форматирования, выявление забытых проверок ошибок, поиск потенциальных утечек и др.) и интегрируется в процесс CI. Интеграция golangci-lint позволяет на раннем этапе обнаруживать дефекты и поддерживать единый стандарт качества кода.
\end{itemize}

\subsubsection*{gRPC и отказ от REST}

Межсервисное взаимодействие организовано с использованием gRPC, что обеспечивает бинарную сериализацию данных (Protocol Buffers) и строгое определение контрактов. В отличие от REST, gRPC передаёт сообщения в компактном бинарном формате по протоколу HTTP/2, что снижает сетевые задержки и уменьшает объём передаваемых данных. Определение интерфейсов и типов сообщений в файлах .proto задаёт жёсткую схему запросов и ответов, благодаря чему генерируемый Go-код точно соответствует контракту. Это позволяет компилятору выявлять несоответствия на этапе сборки и предотвращает ошибки интерфейса между сервисами.

\begin{itemize}
    \item Бинарная сериализация и эффективность: gRPC использует HTTP/2 и Protocol Buffers, что даёт более компактные сообщения и эффективную сериализацию по сравнению с текстовым JSON. Меньший объём данных и мультиплексирование запросов на одном соединении приводят к низкой задержке и высокой пропускной способности, особенно при большом числе мелких вызовов.
    \item Строгие контракты и стабильность API: Определение сервисов в .proto-файлах задаёт чёткие схемы запросов и ответов. Для Go применяются плагины protoc-gen-go и protoc-gen-go-grpc, автоматически генерирующие типобезопасные структуры, сервисные интерфейсы и клиентские/серверные заглушки. При внесении изменений компилятор быстро выявляет возможные несовместимости, что повышает надёжность и контролируемость версий API.
    \item Двунаправленные стримы: gRPC поддерживает как однонаправленные, так и двунаправленные потоки данных. Это позволяет серверам и клиентам обмениваться сообщениями в режиме реального времени и передавать данные по мере их готовности.
    \item Отказ от REST: gRPC выбран вместо REST ввиду строгого подхода к описанию интерфейсов и высокой производительности. REST с JSON-представлением создаёт дополнительные расходы на сериализацию и десериализацию и требует ручной валидации данных, что увеличивает задержки при большом числе запросов. В тесно интегрированной микросервисной среде эти накладные расходы могут стать узким местом, тогда как строго типизированный и генерируемый gRPC обеспечивает масштабируемость и упрощает сопровождение системы.
\end{itemize}

\subsubsection*{Логирование}

Интерфейс логгера основан на библиотеке zap (Uber) и обёрнут в собственный модуль. Такой подход обеспечивает высокую производительность и структурированный вывод логов (каждое сообщение содержит уровень, метку времени и дополнительные поля). Ротация логов организована с помощью пакета lumberjack: при достижении заданного размера или срока хранения текущий файл переименовывается и создаётся новый, что предотвращает бесконтрольный рост логов. Логи записываются одновременно в стандартный поток stdout (важно для контейнеризированной среды) и в файлы, что упрощает как оперативный мониторинг (например, сбор логов через оркестратор), так и долговременное хранение журналов.

\begin{itemize}
    \item Обёртка над zap: высокопроизводительное и структурированное логирование с возможностью добавления метаданных (уровень, модуль, временная метка и т. д.).
    \item Lumberjack: пакет для ротации лог-файлов по размеру и времени (архивирование устаревших файлов).
    \item Одновременный вывод: запись логов как в stdout (для сбора логов контейнеризатором), так и в файл (для архивации и анализа).
\end{itemize}

Конфигурация сервисов хранится в YAML-файлах и загружается с помощью gopkg.in/yaml.v3. Формат YAML выбран из-за его читабельности и поддержки вложенных структур и комментариев. Библиотека yaml.v3 десериализует данные конфигурации (например, параметры баз данных, порты, флаги) прямо в структуры Go (через теги yaml), что обеспечивает строгую типизацию и валидацию. Благодаря единому YAML-файлу все параметры сосредоточены в одном месте, легко читаются и изменяются без сложной сериализации или форматирования.

\begin{itemize}
    \item Формат YAML: человекочитаемый, поддерживает вложенность и комментарии, упрощает ручную правку конфигов.
    \item gopkg.in/yaml.v3: преобразует YAML в Go-структуры по тегам yaml, обеспечивая строгую проверку и удобный доступ к параметрам.
\end{itemize}

\subsubsection*{PostgreSQL и Redis}

Для работы с PostgreSQL используется стандартный пакет database/sql вместе с драйвером (pgx или pq). Такой подход даёт полный контроль над SQL-запросами и высокую прозрачность операций: все SQL-команды пишутся вручную, что позволяет точно понимать, какие запросы выполняются, и оптимизировать их при необходимости. Отсутствие ORM снижает «магический» код и упрощает отладку. Транзакции и подготовленные выражения (PreparedStatement) позволяют эффективно управлять соединениями и защищать от SQL-инъекций.

\begin{itemize}
    \item Контроль и прозрачность: database/sql предоставляет гибкость и прямой контроль над запросами (сложные JOIN, CTE, индексы и т.д.), упрощая оптимизацию и диагностику производительности.
    \item Гибкость: без ограничений ORM можно использовать все возможности PostgreSQL; результаты запросов считываются в типизированные структуры Go.
\end{itemize}

Схема базы данных поддерживается через миграции: каждая версия представлена парой файлов *.up.sql и *.down.sql. Файлы up содержат команды для применения изменений (создание или изменение таблиц и др.), а down — инструкции для отката этих изменений. Миграции нумеруются или метятся по времени, обеспечивая последовательность. Для применения миграций используется утилита migrate, часто запускаемая через Docker. Команда migrate up последовательно применяет все новые скрипты, а migrate down откатывает последние изменения. Такой механизм гарантирует синхронизацию структуры БД во всех средах и позволяет безопасно возвращаться к предыдущей версии.

Redis выполняет роль in-memory хранилища для временных данных и кэша. Для ключей устанавливается время жизни (TTL), что удобно для одноразовых или временных объектов: например, SMS-коды верификации, одноразовые токены или кэшированные результаты. По истечении TTL ключ автоматически удаляется, что упрощает управление памятью. Также ключи разделяются префиксами (например, verification:, cache:, session:) для группировки по функциональности и предотвращения коллизий имён. Redis отлично подходит для сценариев с высокой частотой чтения и короткоживущих данных, например, хранения кодов верификации или кэширования результатов интенсивных запросов.

\begin{itemize}
    \item TTL: ключи с ограниченным сроком жизни; подходит для временных данных (верификационных кодов, сессий), которые автоматически удаляются.
    \item Префиксы ключей: группировка ключей по префиксам (otp:, cache: и др.) упрощает организацию пространства имён и избегает пересечений.
\end{itemize}

\subsubsection*{NATS JetStream и брокер}

Обычный NATS — это лёгкий брокер сообщений с поддержкой pub/sub, но без долговременного хранения: сообщения доставляются активным подписчикам мгновенно и затем удаляются (at-most-once). JetStream расширяет возможности NATS, добавляя постоянное хранилище и подтверждение доставки (at-least-once). Сообщения сохраняются на диске в потоках (streams), и потребители должны отправлять подтверждение (Ack). Если сообщение не подтверждено, оно остаётся в потоке и будет доставлено повторно. Благодаря JetStream новые или временно недоступные сервисы могут получить все пропущенные события после восстановления подключения, что обеспечивает согласованность данных в микросервисной архитектуре.

\subsection*{6.2 Реализация микросервисов}
\addcontentsline{toc}{subsection}{6.2 Реализация микросервисов}

Система спроектирована по принципу слабосвязанной, модульной архитектуры: каждый сервис отвечает за отдельную доменную область и может масштабироваться независимо. Обмен сообщениями между сервисами осуществляется через gRPC-вызовы (все гRPC-запросы маршрутизируются через API-Gateway Envoy), а асинхронные события обрабатываются посредством брокера сообщений NATS JetStream. Хранилищем данных выступают PostgreSQL (для большинства сервисов), Redis (для кэша и хранения токенов), MinIO (S3-совместимое хранилище файлов); внешним почтовым сервисом служит Brevo (SendinBlue) для отправки электронной почты.

\subsubsection*{Auth Service}
Назначение: Сервис аутентификации и авторизации отвечает за регистрацию пользователей, проверку их учётных данных и управление JWT-токенами (OAuth2.0). Он обеспечивает создание новых учётных записей, выдачу и обновление токенов доступа и обновления (refresh tokens), а также валидацию токенов при запросах к другим сервисам.

Основные бизнес-задачи: регистрация (SignUp), вход (SignIn), выход (Logout), обновление токенов (RefreshToken), проверка валидности токена (ValidateToken). При регистрации выполняется проверка уникальности email, хеширование пароля (bcrypt/Argon2), сохранение пользователя в БД и установка статуса учётной записи. При успешной регистрации сервис публикует событие о новом пользователе в шину NATS JetStream (топик \\ EmailEventsService) для последующей отправки приветственного письма через сервис уведомлений. При логине проверяются email и пароль пользователя, после чего генерируются JWT (доступа) и токен обновления. Сами refresh-токены сохраняются в Redis (Auth Cache) с привязкой к пользователю для возможности проверки/отзыва. При запросе на обновление токена сервис проверяет валидность переданного refresh-токена (существует ли он в Redis и не истёк ли), генерирует новые токены. При logout — удаляет refresh-токен из Redis или помечает его аннулированным. Для повышения безопасности сервис ограничивает длину и формат входных данных, возвращая INVALID\_ARGUMENT, если аргументы некорректны.

gRPC-методы и логика:
\begin{itemize}
    \item RegisterUser (регистрация): принимает email и пароль. Сначала проверяет, что email имеет корректный формат и в БД пока нет пользователя с таким email. Если дублирование обнаружено, возвращает ошибку ALREADY\_EXISTS. В случае отсутствия конфликта создаёт запись в таблице users (см. рис. 3.3–2) и устанавливает статус пользователя «active». После сохранения отправляет событие в NATS для рассылки email. Возвращает статус OK или INTERNAL\_ERROR при ошибке БД.
    \item LoginUser (вход): получает email и пароль. Ищет пользователя в БД; если не найден — возвращает NOT\_FOUND; если пароль неверен — возвращает UNAUTHENTICATED. При успехе генерирует JWT и refresh-токен, сохраняет refresh-токен в Redis (TTL согласно настройкам) и возвращает токены.
    \item RefreshToken: на вход получает старый refresh-токен. Проверяет его наличие в Redis: если отсутствует или истёк — возвращает \\ UNAUTHENTICATED, иначе удаляет старый и выдаёт новую пару токенов (новый refresh также сохраняется в Redis).
    \item ValidateToken: принимает JWT, декодирует и проверяет подпись и срок. Если токен невалиден или истёк — возвращает UNAUTHENTICATED, иначе возвращает успех.
    \item При необходимости могут быть методы Logout (удаление refresh-токена) и RevokeToken. Все методы обрабатывают ошибки невалидных аргументов (INVALID\_ARGUMENT), ошибки БД (INTERNAL) и нарушения бизнес-правил (дублирование — ALREADY\_EXISTS, отсутствие ресурса — NOT\_FOUND).
\end{itemize}
Архитектура и организация кода: Сервис реализован на Go по принципам «чистой архитектуры». Слои разделены следующим образом: уровень Domain описывает сущность User и её бизнес-логику, уровень Repository (интерфейс и реализация) взаимодействует с PostgreSQL и Redis, уровень Usecase реализует сценарии (регистрация, логин и др.), слой Delivery содержит gRPC-сервер и переводит входящие протоколы Protobuf в доменные структуры. В корне пакета определены интерфейсы (файлы repository.go, usecase.go), а в подпакетах (/repository, /usecase, /delivery) находятся их реализации и обработчики. При запуске сервиса происходит конфигурация соединений с БД и Redis, настройка gRPC-сервера (регистрация методов AuthServiceServer из protobuf), а также настройка клиента NATS для публикации событий. Все зависимости (доступ к БД, брокеру сообщений и прочему) внедряются через параметры конструкторов (Dependency Injection) для слабой связанности и удобства тестирования.

Работа с базой и кэшем: AuthService использует PostgreSQL для хранения учётных записей и статус пользователей. Для хранения активных refresh-токенов применяется Redis, позволяющий быстро валидировать токены и осуществлять их отзыв (удаление при logout). Таблица users имеет уникальный индекс по полю email — при попытке регистрации существующего email будет выброшена ошибка ALREADY\_EXISTS (код 6).

Взаимодействие с другими сервисами: после создания учётной записи AuthService генерирует событие о регистрации и публикует его в NATS JetStream (топик EmailEventsService). Сервис Notifications подписан на этот топик и при получении события отправляет пользователю письмо через Brevo. Также сервис Auth может выполнять вызов CreateProfile в ProfileService (гRPC-вызов profile.ProfileService), чтобы автоматически создать связанную запись профиля при регистрации (например, с начальным именем и аватаром по умолчанию). Такой вызов производится синхронно по gRPC и при возникновении ошибок взаимодействия возвращает FAILED\_PRECONDITION или UNAVAILABLE. При аутентификации запросы клиентов требуют действительного JWT, который проверяется во всех сервисах через встроенную логику AuthService (или встроенным middleware), генерируя UNAUTHENTICATED для невалидного/просроченного токена.

\subsubsection*{Profile Service}
Назначение: Сервис профилей отвечает за хранение и управление пользовательскими профилями, включая информацию о имени (username), ФИО, дате рождения, аватаре, а также за реализацию подписок (follow/unfollow) между пользователями.

Основные бизнес-задачи: хранение данных профиля, поддержка уникальности username, обработка действий «подписаться/отписаться» между пользователями. Сюда входят операции создания/чтения/обновления/удаления профиля (CRUD), изменение аватара, поиск профилей, а также ведение таблицы подписчиков. При создании нового профиля сервис проверяет, что username уникален (уникальный индекс), и сохраняет новую запись, связывая её с user\_id из AuthService. При обновлении профиля возможно изменение поля username (тоже с проверкой уникальности) и загрузка нового аватара. Для загрузки/получения файла аватара сервис обращается к FileStorageService через gRPC. Например, метод UploadAvatar (реализованный во FileStorage) вызывается из ProfileService: он возвращает ID файла, который затем сохраняется в поле profiles.image\_id. Поля followers\_count и following\_count автоматически обновляются при операциях подписки.

В качесте базы данных используется PostgreSQL с двумя таблицами: таблица profiles (поля user\_id (PK,UUID), username, first\_name, last\_name, \\ date\_of\_birth, image\_id, счетчики подписок, метки времени) и связная таблица follows (follower\_id, followed\_id). Поле username имеет уникальное ограничение, поэтому в случае конфликта возвращается ALREADY\_EXISTS.

gRPC-методы и логика:
\begin{itemize}
    \item CreateProfile: получает user\_id (из Auth), username, ФИО и др. Проверяет заполнение обязательных полей; если username занят — возвращает ALREADY\_EXISTS. Далее создает запись в profiles. Ошибки БД обрабатываются как INTERNAL.
    \item GetProfile: по user\_id или username извлекает данные профиля. Если профиль не найден — возвращает NOT\_FOUND. Может возвращать публичную информацию (без паролей).
    \item UpdateProfile: обновляет имя, фамилию, дату рождения; при смене \\ username повторяет проверку уникальности (ALREADY\_EXISTS при дублировании). Если указана загрузка нового аватара, получает от клиента поток байтов и передает его во FileStorageService (см. ниже). При успехе возвращает обновленный профиль. Ошибки валидации аргументов дают INVALID\_ARGUMENT.
    \item DeleteProfile: удаляет запись профиля и связанные данные (например, обнуляет подписки). Если профиль не найден — NOT\_FOUND.
    \item FollowUser: принимает follower\_id и followed\_id. Проверяет, что оба профиля существуют (вызов GetProfile). Нельзя подписаться на себя — приводит к FAILED\_PRECONDITION. Если подписка уже существует, возвращает ALREADY\_EXISTS. Иначе создает запись в таблице follows и инкрементирует счетчики followers\_count/following\_count в таблице profiles.
    \item UnfollowUser: обратная операция; если записи нет — возвращает\\ NOT\_FOUND, иначе удаляет подписку и декрементирует счетчики.
    \item ListFollowers/ListFollowing: возвращают список пользователей по запросу; возвращают пустой список, если подписчиков нет.
    \item UploadAvatar (в сотрудничестве с FileStorageService): клиент передаёт бинарный поток файла; ProfileService может выступать прокси или просить FileStorageService сгенерировать presigned URL. После загрузки файл регистрируется в таблице files FileStorage, а ID сохраняется в profiles.image\_id. При некорректных файлах (слишком большой размер, неподдерживаемый формат) возвращается INVALID\_ARGUMENT или RESOURCE\_EXHAUSTED.
\end{itemize}
Сервис разбит по «чистой архитектуре» на слои: доменная модель Profile (domain), интерфейсы репозитория и usecase (catalog of business logic), реализации (persistence с использованием SQL), а также слой доставки (гRPC handlers). Например, в корне модуля определён интерфейс \\ ProfileRepository (CRUD-операции над profiles/follows) и интерфейс ProfileUsecase с методами вроде CreateProfile, FollowUser и т. д., а соответствующие имплементации находятся в подпакетах repository и usecase. Слой Delivery реализует gRPC-сервер (generated из protobuf) и преобразует входящие запросы в вызовы usecase. Такой подход гарантирует независимость бизнес-логики от конкретного хранилища и протокола, упрощает тестирование и замену компонентов.

Взаимодействие: ProfileService получает user\_id от AuthService (обычно через метаданные gRPC/токен). При создании профиля AuthService инициирует gRPC-вызов CreateProfile в ProfileService для добавления записи. \\ ProfileService обращается к FileStorageService (filestorage.FileStorageService) для работы с аватаром: при загрузке пользователя на уровне usecase вызывается метод для генерации PresignedURL или непосредственной передачи файла. Ответы от FileStorage возвращают либо ID файла, либо ошибку (NotFound, если хранилище недоступно). ProfileService не публикует событий в NATS напрямую (логика подписок обрабатывается синхронно), однако при необходимости можно расширить систему на события (например, оповещения через \\ Notifications при новых подписках).

\subsubsection*{Activity Service}
Назначение: Сервис активности реализует функциональность «лайки», «комментарии» и подсчета статистики по контенту. Он связывает пользователей и туристический контент (маршруты) в терминах социального взаимодействия.

Основные бизнес-задачи: добавление/удаление лайков к маршруту, публикация комментариев к маршруту, асинхронное обновление счётчиков лайков/комментариев. Например, пользователь может поставить лайк маршруту — ActivityService сохраняет соответствующую запись в БД. Аналогично создаются записи комментариев. Сервис также обеспечивает получение списка комментариев и информации о лайках. В системе реализовано асинхронное обновление агрегированных счётчиков: после изменения лайка или комментария сервис публикует событие в NATS, которое обрабатывает вспомогательный компонент (ActivityCountersService) для обновления общих счетчиков в БД ContentService. Это позволяет снизить нагрузку и обеспечить eventual consistency.

База данных: PostgreSQL с таблицами: route\_likes (поля route\_id, user\_id, метка и времени): и route\_comments (comment\_id PK, route\_id, user\_id, text, created\_at). Каждая запись лайка уникальна по паре (route\_id, user\_id). При попытке поставить повторный лайк должна сработать проверка уникальности. 

gRPC-методы и логика:
\begin{itemize}
    \item AddLike: получает route\_id и user\_id. Сервис сначала проверяет, что маршрут существует (вызывая метод GetRoute из ContentService); если нет — возвращает NOT\_FOUND. Затем пытается вставить запись в route\_likes. Если такая запись уже есть (пользователь уже лайкал) — возвращает ALREADY\_EXISTS. Иначе сохраняет лайк и публикует событие LikeAdded в NATS JetStream (содержит route\_id), чтобы обновить счётчик лайков.
    \item RemoveLike: удаляет запись лайка. Если записи нет — NOT\_FOUND. После удаления публикуется событие LikeRemoved.
    \item AddComment: принимает route\_id, user\_id, текст. Проверяет существование пользователя (ProfileService) и маршрута (ContentService). Вставляет запись в route\_comments с новым comment\_id. Если текст пустой или слишком длинный — INVALID\_ARGUMENT. При успехе публикует событие CommentAdded.
    \item DeleteComment: удаляет свой комментарий (по comment\_id). Если не найден или не принадлежит пользователю — NOT\_FOUND или \\ PERMISSION\_DENIED.
    \item ListComments: возвращает список комментариев по маршруту. Если маршрута нет — NOT\_FOUND.
    \item GetLikeCount/GetCommentCount: может возвращать текущие счётчики. Эти данные можно либо поддерживать в памяти (посчитать при запросе), либо получать из агрегированной таблицы/кэша, обновляемой с помощью NATS-событий.
\end{itemize}
В упомянутых методах используются стандартные коды ошибок gRPC: \\ NOT\_FOUND при отсутствии ресурса, ALREADY\_EXISTS при попытке дублирования (например, лайка), INVALID\_ARGUMENT при ошибках валидации, \\ если пользователь не авторизован — UNAUTHENTICATED,  и PERMISSION\_DENIED при попытке удалить чужой комментарий.

Архитектура: Чистая архитектура: доменные сущности «Like» и «Comment», репозиторий для CRUD-операций над таблицами (PostgreSQL), usecase-слой для бизнес-логики (проверка существования маршрута/пользователя, транзакции вставки/удаления, публикация событий), слой доставки gRPC. В модуле определены интерфейсы ActivityRepository и ActivityUsecase, их реализации и gRPC-хендлеры. Очистка кода и тестируемость обеспечиваются тем, что работа с БД и внешними сервисами (ContentService, ProfileService, NATS) инвертирована через интерфейсы.

Работа с NATS: Каждый раз при изменении лайков или комментариев сервис публикует событие в JetStream. Например, после успешной вставки лайка отправляется сообщение «LikeAdded» (с JSON- или Protobuf-данными). Другой поток (ActivityCountersService) подписан на эти события и обновляет агрегированные счётчики в таблице маршрутов или отдельном хранилище. Такой подход гарантирует, что тяжёлые операции обновления статистики выполняются асинхронно, а базовая транзакция обработки лайка/комментария остаётся простой и быстрой. 

Взаимодействие: при добавлении лайка или комментария ActivityService обращается к ContentService (content.ContentService) для проверки существования маршрута, а к ProfileService (profile.ProfileService) для проверки пользователя (определение его прав). Эти вызовы gRPC инкапсулированы в usecase (например, интерфейсы ContentClient, ProfileClient). Обмен сообщениями с FileStorageService или Notifications здесь не происходит напрямую (если только при расширении системы делать уведомления об активности).

\subsubsection*{Content Service}
Назначение: Сервис контента отвечает за хранение и управление туристическим контентом: маршрутами (routes) и достопримечательностями/местами (places). Он предоставляет CRUD-операции над маршрутами (название, сложность, дистанция, координаты точек) и местами (имя, адрес, описание, координаты). Кроме того, организована поддержка связывания мест с маршрутами в заданном порядке и прикрепления мультимедийных файлов (изображений) к этим сущностям.

Основные бизнес-задачи: добавление, обновление, удаление и поиск маршрутов и мест, фильтрация маршрутов по параметрам (например, сложности), изменение последовательности точек пути. Каждому маршруту может соответствовать несколько мест (через промежуточную таблицу route\_places), и у каждого места может быть несколько изображений (таблица place\_files). Сервис обеспечивает консистентное хранение геоданных (latitude/longitude) и поддерживает транзакции при изменении маршрутов и связанных объектов. Также реализован gRPC метод потоковой передачи (stream) для загрузки/выгрузки изображений: например, клиент может вызывать метод \\ UploadPlaceImage как поток данных, который на стороне сервиса перенаправляется в \\ FileStorageService для сохранения в MinIO.

База данных: PostgreSQL с таблицами (см. рис. 3.3–5):
\begin{itemize}
    \item routes (id UUID, name, difficulty, distance, массивы координат \\ latitudes/longitudes, user\_id владельца, метки времени, описание).
    \item places (id UUID, name, address, description, latitude, longitude, метки времени).
    \item Связующая таблица route\_places (route\_id, place\_id, ordering для порядка следования).
    \item Таблица files (используется FileStorageService) и place\_files (place\_id, file\_id, ordering) для привязки файлов-изображений к месту.
\end{itemize}
Для каждой таблицы определены первичные ключи и необходимые индексы. В таблице routes поле name может иметь составной уникальный индекс (например, уникальность имени маршрута для одного пользователя). Схема организации данных позволяет эффективно фильтровать и выполнять геозапросы (через SQL или PostGIS расширения).

gRPC-методы и логика:

Маршруты:
\begin{itemize}
    \item CreateRoute: принимает параметры маршрута (имя, сложность, дистанцию, координаты, описание, user\_id). Проверяет обязательные поля (имя, хотя бы одну координату и пр.); если имя уже занято у данного пользователя — возвращает ALREADY\_EXISTS. Создаёт новую запись в routes, сохраняет массивы координат. При добавлении начальных мест может выполнять дополнительные вставки в route\_places.
    \item GetRoute: по route\_id возвращает структуру маршрута со всеми полями и списком связанных мест (с учётом поля ordering). Если маршрут не найден — NOT\_FOUND.
    \item UpdateRoute: обновляет свойства маршрута (может менять имя, сложность, описание и т. д.); проверяет наличие записи; если маршрут с новым именем конфликтует — ALREADY\_EXISTS. Для изменения путевых точек может реализовываться несколько методов (например, \\ SetRoutePlaces), которые обновляют route\_places (добавление/удаление мест).
    \item DeleteRoute: удаляет маршрут и все связанные записи (места к маршруту, файлы). При отсутствии записи — NOT\_FOUND.
    \item ListRoutes: возвращает список маршрутов с возможными фильтрами (например, по сложности, по географическому диапазону). Если фильтр указан некорректно — INVALID\_ARGUMENT.
\end{itemize}
Места (Places):
\begin{itemize}
    \item CreatePlace/GetPlace/UpdatePlace/DeletePlace: аналогичные \\ CRUD-операции для объектов places. При создании указывается имя, адрес, координаты.
    \item AddPlaceToRoute/RemovePlaceFromRoute: связывают место с маршрутом. Если route\_id или place\_id не найдены — NOT\_FOUND. При добавлении проверяется, что такая связь ещё не существует (иначе \\ ALREADY\_EXISTS), и задаётся порядок следования.
\end{itemize}
Изображения (Media):
\begin{itemize}
    \item UploadPlaceImage (stream): потоковое получение бинарных данных изображения от клиента. ContentService принимает потоки (Chunked gRPC) и переадресует данные во FileStorageService: либо получает от него Presigned URL (и сам загружает через HTTP), либо напрямую вызывает UploadFile gRPC. После успешной загрузки сервис получает идентификатор файла (из FileStorage) и создаёт запись в place\_files с place\_id и file\_id. Если файл по размеру превышает лимит — \\ RESOURCE\_EXHAUSTED.
    \item GetPlaceImage (stream): загрузка изображения с помощью gRPC-стрима из FileStorageService по file\_id и транслирование клиенту. Если файл отсутствует — NOT\_FOUND.
\end{itemize}
Таким образом, большинство gRPC-методов ContentService оборачивает стандартные CRUD-операции над SQL, дополняя их необходимыми проверками и вызовами внешнего сервиса для работы с файлами. При этом используется унифицированный паттерн обработки ошибок: NOT\_FOUND для отсутствующих записей, ALREADY\_EXISTS для дубликатов, INVALID\_ARGUMENT при некорректных входных данных.

Архитектура: как и в других сервисах, применена «чистая архитектура»: в пакете content определены доменные модели Route и Place, интерфейсы репозитория и usecase (файлы repository.go, usecase.go на верхнем уровне) и их реализации в подпакетах. Layer usecase инкапсулирует основную логику (создание маршрутов, валидация координат, связывание с файлами), слой repository (например, PostgresRepository) выполняет SQL-запросы к таблицам. Слой Delivery реализован gRPC-сервер: все обработчики принимают Protobuf-запросы и вызывают методы usecase. Такой подход отделяет бизнес-логику от инфраструктуры (БД, протокол коммуникации).

Работа с файловым сервисом и хранением: для работы с изображениями ContentService взаимодействует с FileStorageService. В момент загрузки изображения сервис запрашивает у FileStorage presigned URL или использует gRPC-метод загрузки. Ответ от FileStorage возвращает file\_id, который связывается с place\_id в таблице place\_files. MinIO настроен как S3-совместимое хранилище; FileStorage контролирует TTL для файлов и шифрование. API-Gateway (Envoy) перенаправляет HTTP-запросы по presigned URL напрямую в MinIO, что разгружает сервис от передачи больших бинарных файлов.

Взаимодействие с другими сервисами: ContentService может вызывать методы ProfileService (например, для получения информации о владельце маршрута) и ActivityService (для получения количества лайков/комментариев), но в минимальной конфигурации такого нет. В основном другие сервисы обращаются к ContentService: ActivityService — для проверки существования маршрута, ProfileService — при необходимости искать маршруты по user\_id, AuthService — при проверке прав владельца, и FileStorageService — для загрузки файлов. После модификации контента сервис может публиковать события (например, «RouteCreated» или «PlaceAdded») в NATS для дальнейшей обработки или обновления кэшей, однако базовые операции сконцентрированы на синхронной CRUD-логике.

\subsubsection*{FileStorage Service}
Назначение: Сервис управления файлами обеспечивает надёжную загрузку, хранение и выдачу медиафайлов (изображений, аватаров и т. д.) в систему. Он выступает как прослойка между микросервисами и объектным хранилищем MinIO, реализуя политику безопасности, ограничения по размеру, TTL (время жизни) и шифрование файлов.

Основные бизнес-задачи: приём файлов от клиентов или других сервисов, сохранение их в MinIO, хранение метаданных в БД, выдача файлов по запросу, управление сроком жизни (TTL) и правами доступа. Также FileStorage генерирует presigned URL, позволяющие клиентам напрямую загружать или скачивать файлы через MinIO по HTTP (через API Gateway).

База данных: PostgreSQL с таблицей files (пример на рис. 3.3–3 и 3.3–5). Поля: id (integer, PK), file\_name, storage\_bucket, storage\_id (идентификатор в MinIO), internal\_url, placeholder (для превью), size, флаги доступа (from\_public), метки времени. Эта таблица хранит метаданные загруженных файлов. Первичный ключ — целочисленный id.

gRPC-методы и логика:
\begin{itemize}
    \item UploadFile (stream): клиент передаёт байтовый поток файла. Сервис накапливает полученные данные, проверяет, чтобы общий размер не превышал предельный (например, 10 MB); если превышает, прерывает с кодом RESOURCE\_EXHAUSTED. Данные шифруются на лету (используется серверная сторона MinIO или KMS) и отправляются в заданный бакет MinIO. После завершения загрузки генерируется запись в таблице files с метаданными (имя, размер, internal\_url и пр.) и возвращается id файла. Если запись с таким именем уже есть — ALREADY\_EXISTS. При любых внутренних ошибках возвращается INTERNAL.
    \item DownloadFile (stream): клиент запрашивает файл по id. Сервис находит запись в БД; если нет — NOT\_FOUND. Далее запрашивает файл из MinIO и передаёт данные клиенту по gRPC-стриму. Если файл удалён или повреждён — возвращает NOT\_FOUND или INTERNAL.
    \item GetPresignedUploadURL: по запросу с названием файла возвращает клиенту подписанный URL для загрузки (HTTP PUT) непосредственно в MinIO. Срок действия URL ограничен TTL (например, 15 минут). Это позволяет разгрузить сервис от передачи больших файлов через gRPC.
    \item GetPresignedDownloadURL: аналогично выдаёт URL для скачивания (HTTP GET).
    \item DeleteFile: удаляет запись из БД и файл из MinIO. Если файла нет — NOT\_FOUND.
\end{itemize}
Архитектура: Сервис организован по слоям: доменная модель File с методами (например, метод проверки размера), репозиторий для работы с PostgreSQL, usecase-слой, и слой доставки gRPC. Используются интерфейсы для абстракции работы с MinIO (например, FileStorageClient) и для доступа к БД. Шифрование файлов осуществляется через встроенные механизмы MinIO (Server-Side Encryption) или путем шифрования байтов перед отправкой. Политика TTL задаётся либо на стороне MinIO (правила удаления после X дней), либо самим сервисом (отмечая метку времени в БД и периодически удаляя устаревшие файлы, используя внутренний scheduler или события NATS).

Интеграция с другими сервисами: ProfileService и ContentService обращаются к FileStorageService при необходимости загрузки или получения файлов: они могут использовать gRPC-методы загрузки либо presigned URL. Например, при смене аватара ProfileService запрашивает GetPresignedUploadURL, получает ссылку и отправляет файл напрямую в MinIO. Либо вызывает UploadFile с передачей потока. После получения file\_id записывает его в профиль. \\ ContentService поступает аналогично для изображений мест. API Gateway (Envoy) перенаправляет запросы по presigned URL напрямую в MinIO. FileStorageService также публикует события об удалении старых файлов в NATS (для логирования или обратного оповещения других компонентов). Ошибки взаимодействия (например, NOT\_FOUND при запросе несуществующего файла) обрабатываются по стандарту gRPC.

\subsubsection*{Notifications Service}
Назначение: Сервис уведомлений занимается отправкой электронных сообщений пользователям системы через сторонний SMTP/API сервис Brevo (SendinBlue). Он реагирует на события, публикуемые другими микросервисами в NATS JetStream, и превращает их в email-уведомления (регистрация, смена пароля, действия в приложении и т. д.).

Основные бизнес-задачи: подписка на топики событий NATS (например, UserRegistered, PasswordReset, CommentAdded), формирование писем по заранее заданным шаблонам и отправка их через REST API Brevo. При получении события сервис десериализует данные, проверяет их корректность (если формат события неверен — INVALID\_ARGUMENT) и, в случае необходимости, обогащает информацией из ProfileService (например, подставляет имя пользователя в письмо). После подготовки вызывает Brevo API; при проблемах (недоступность сети, неверные настройки) логирует ошибку и может инициировать повторную отправку (алгоритм повторов настраивается отдельно). Код ответа внешнего сервиса не влияет на gRPC-интерфейс, так как Notifications работает по модели Pub/Sub, а не по gRPC-запросам.

Архитектура: внутри NotificationsService также используется концепция чистой архитектуры: есть слой получателей сообщений (Subscriber) от NATS, слой бизнес-логики (формирование тела письма, выбор шаблона) и слой доставки (REST-клиент к Brevo). Хранилища данных как такового нет (все шаблоны и конфигурации хранятся в конфиге или внутрипамяти). Каждый subscriber запускается как отдельный поток, подписывается на соответствующий topic JetStream и вызывает usecase при получении сообщения. Такая модульная организация упрощает добавление новых типов уведомлений.

Взаимодействие: NotificationsService не предоставляет собственных \\ gRPC-методов клиентам; его задача — слушать события от других сервисов. Например, после регистрации AuthService публикует событие «UserRegistered» (с email и именем), Notifications его получает и вызывает Brevo API. Подобным образом могут обрабатываться события из ActivityService (например, «CommentAdded» для уведомления автора маршрута) или любые другие. Благодаря использованию NATS и JetStream достигается надёжная асинхронная доставка: если сервис упадёт, после перезапуска он продолжит получать неполученные сообщения.

Коды ошибок и устойчивость: NotificationsService возвращает в лог или мониторинг статусы успеха/ошибок API Brevo (например, 502 Bad Gateway при недоступности Brevo трактуется как UNAVAILABLE). В gRPC-слое также можно определить метод SendEmail (например, для ручной отправки), который на вход принимает структуру EmailRequest и возвращает статус. В таком случае возвращаемые коды будут аналогичны: INVALID\_ARGUMENT на неправильный формат адреса или тела письма, INTERNAL на ошибки сервера. Однако основная работа — через асинхронные события и REST, а не через gRPC.

Таким образом, каждый микросервис спроектирован с учётом принципов модульности, слабой связанности и расширяемости. AuthService обеспечивает безопасность и единую авторизацию, ProfileService — персональные данные пользователей, ActivityService — социальные функции, ContentService — хранение маршрутов, FileStorageService — управление файлами, а \\ NotificationsService — оповещения. Между ними устанавливаются чёткие интерфейсы (gRPC и события NATS), что упрощает сопровождение и масштабирование системы. 

\subsection*{3.3 Организация DevOps-инфраструктуры и CI/CD}
\addcontentsline{toc}{subsection}{3.3 Организация DevOps-инфраструктуры и CI/CD}

\subsubsection*{Структура Helm-чартов}
В системе «Путешествия по России» каждый микросервис (auth, profile, activity, content, filestorage, notifications, broker) оформлен отдельным Helm-чартом. Такой подход обеспечивает модульность и независимость сервисов: каждый чарт описывает Kubernetes-ресурсы для одного приложения. Все эти чарты объединены в один родительский чарт platform, в котором с помощью секции dependencies задаются зависимости на каждый микросервис. В блоке Chart.yaml родительского чарта указываются пары name, version и repository, причём в качестве репозитория используется адрес OCI-реестра (например, GHCR). 

При локальной отладке каждый микросервис может запускаться со своим собственным файлом values.yaml, содержащим настройки окружения для разработки. Для деплоя в production используется централизованный набор конфигураций: общий values.yaml и разделённый на обычные и секретные параметры values.secret.yaml. В этих файлах определяются порты, URL сервисов, переменные окружения, параметры подключения к БД и т. д., а чувствительные данные (пароли, токены) выносятся в values.secret.yaml для безопасности. Кроме того, в чартах сервисов, использующих PostgreSQL, настраивается Kubernetes Job с аннотацией helm.sh/hook: pre-install,pre-upgrade для применения миграций схемы базы данных перед установкой или обновлением приложения. Такой job загружает контейнер с миграциями и выполняет SQL-скрипты, гарантируя, что схема БД актуализируется до запуска основного пода.

\subsubsection*{CI/CD (GitHub Actions)}
В каждом репозитории микросервиса организован конвейер CI/CD на базе GitHub Actions с несколькими workflow-файлами. Обычно используются три типа workflow: app.yaml для сборки приложения и создания Docker-образа, publish-*-chart.yaml для упаковки и публикации Helm-чарта, а при наличии PostgreSQL — migrations.yaml для сборки контейнера с миграциями. Все эти действия выполняются в среде GitHub Actions с использованием секретов (например, GHCR\_TOKEN, GHCR\_USERNAME, GOPRIVATE\_PAT) для аутентификации в GitHub Container Registry и доступа к приватным модулям.

Сборка приложения ведётся с помощью многоступенчатого Dockerfile. На этапе build используется образ golang:1.24, в котором скачиваются зависимости (через go mod) и компилируется исполняемый файл. Затем на финальной стадии берётся минимальный образ alpine:3.19, в который копируется только готовый бинарник. В результате итоговый образ получается очень компактным и безопасным: в нём отсутствуют инструменты сборки и прочие ненужные слои. Такой подход значительно уменьшает размер контейнера и поверхность потенциальных уязвимостей.

Настройки триггеров workflow соответствуют следующей логике:
\begin{itemize}
    \item Push в ветку main → выполняется сборка и публикация Docker-образа приложения.
    \item Push тега вида chart-* → запускается упаковка и публикация Helm-чарта в GHCR.
    \item Изменения в папке migrations → инициируется сборка и публикация контейнера миграций.
\end{itemize}
Все собранные Docker-образы и чарты публикуются в GitHub Container Registry (GHCR), где хранятся как пакеты OCI. Использование GHCR позволяет легко управлять версиями и правами доступа. Каждый релиз получает семантический тег (например, v0.1.12), что обеспечивает трассируемость версий. При необходимости образы и чарты могут быть удалены вручную, иначе GHCR сохраняет их бессрочно, что гарантирует повторяемость окружения.

\subsubsection*{Автоматический деплой (Helm)}
Для автоматического деплоя в репозитории vkr-platform настроен отдельный workflow GitHub Actions. При пуше в ветку main этот workflow выполняет команду helm upgrade --install platform, используя заранее подготовленный kubeconfig (он хранится в GitHub Secrets). Поскольку родительский чарт platform содержит все сервисы в секции dependencies, единственный вызов helm разворачивает все микросервисы сразу. При вызове Helm передаются файлы values.yaml и values.secret.yaml из общего хранилища, в которых заданы порты, URL, переменные окружения, параметры подключения к базе данных и другие настройки. Такой централизованный подход упрощает управление конфигурацией: однократная правка значений в values.yaml отражается на всех сервисах.

Перед запуском каждого сервиса из чарта также выполняются миграции схемы (см. раздел 3.4.1). Например, для сервиса auth определяется Kubernetes Job auth-migrations с аннотацией Helm hook, который применяет SQL-миграции до старта контейнера приложения. Таким образом, структура базы данных гарантированно соответствует версии сервиса. В целом этот pipeline позволяет оперативно и согласованно обновлять всю платформу одной операцией Helm – что повышает надёжность и упрощает откат при необходимости.

\subsubsection*{Логирование}
Во всех микросервисах реализовано структурированное логирование на базе библиотеки zap от Uber. Zap изначально проектировался как очень высокопроизводительный логгер: он обеспечивает «blazing fast» скорость записи и избегает лишних аллокаций при сериализации полей. Например, API SugaredLogger из zap в 4–10 раз быстрее многих аналогичных библиотек. В нашем случае zap обёрнут во внутренний интерфейс логгера приложения. Такая обёртка позволяет при необходимости менять реализацию логирования или дополнять её (например, добавлять новые уровни или форматы) без изменения бизнес-логики.

Логи сервисов пишутся одновременно и в консоль (stdout), и в файл (напр., во внутренний том контейнера). Вывод в stdout соответствует рекомендациям Kubernetes: системный агент kubelet читает эти сообщения и агрегирует их, а разработчики могут просматривать логи через kubectl logs. Запись в файл (например, в монтируемый PersistentVolume) позволяет сохранять полную историю логов и при необходимости передавать её в сторонние системы. Выбранная архитектура лога легко расширяется: в будущем можно подключить отправку логов в ELK/Loki, S3 или другие решения, не меняя код микросервисов.

\subsubsection*{Серверное окружение}
Для развертывания микросервисов проекта «Путешествия по России» была использована арендованная виртуальная машина. Параметры виртуального сервера составляют 4 виртуальных ядра процессора (3,3 ГГц), 8 ГБ оперативной памяти и 80 ГБ NVMe SSD-диска. Для доступа к кластеру был выделен статический IP-адрес. Виртуальная машина выступает хост-нодой локального Kubernetes-кластера, в котором развернуты все микросервисы. При запуске всех компонентов в режиме ожидания (без нагрузки) потребление оперативной памяти составляет около 1,5 ГБ.

\subsection*{3.4 Тестирование микросервисной системы}
\addcontentsline{toc}{subsection}{3.4 Тестирование микросервисной системы}

\subsubsection*{Юнит-тестирование}
В Go для написания юнит-тестов используется стандартный пакет testing. Командой go test ./... можно автоматически обнаруживать и выполнять все тесты в проекте.

Дополнительно применяются расширения из библиотеки Testify: пакеты assert и require позволяют делать проверки более выразительными и удобными. Например, assert.Equal(t, ожидаемое, фактическое) упрощает формирование читаемых сообщений об ошибках.

В юнит-тестах проверяются изолированные логические функции микросервисов, не зависящие от внешних компонентов. Сюда относятся задачи валидации данных, генерации и проверки токенов, базовая бизнес-логика «чистых» сервисов и пр. Такие тесты пишутся без обращения к базе данных или сторонним сервисам.

Основная выгода юнит-тестирования – быстрая отладка логики без поднятия всей системы. Это помогает обнаруживать и устранять ошибки на ранних этапах разработки, когда локализовать проблему проще и дешевле. Юнит-тесты гарантируют надёжность каждого компонента как отдельного блока, что повышает устойчивость всей микросервисной архитектуры.

\subsubsection*{Интеграционное тестирование}
Интеграционные тесты проверяют совместную работу сервисов в условиях, близких к реальным. Они направлены на верификацию взаимодействия компонентов и потоков данных между ними, чтобы убедиться, что система функционирует как единое целое. В контексте микросервисов это означает отправку реальных запросов между сервисами через API-шлюз (Envoy) и проверку обработки сообщений через шину (NATS JetStream).
\begin{itemize}
    \item Для тестирования взаимодействия используется Postman Collection, включающая все gRPC- и REST-эндпоинты микросервисов (например, запросы к сервисам Auth, Profile, Content, Notifications). Это позволяет задать последовательность сценариев и проследить, как ответы одного сервиса поступают в следующий.
    \item Запуск коллекции автоматизирован через Postman CLI (Newman) или встроенный Runner. Newman – это инструмент командной строки для выполнения Postman коллекций, который легко интегрируется в CI/CD-пайплайн. К примеру, в Kubernetes можно запустить Postman-контейнер с Newman внутри кластера, чтобы тестировать внутренние сервисы без их внешней экспозиции.
    \item Само тестовое окружение развертывается с помощью Helm-чарта (например, в dev-кластере). Команда helm test позволяет запускать произвольные Kubernetes-ресурсы, включая контейнеры с Newman и тестовыми скриптами. Например, можно использовать официальный образ postman/newman и передавать в него JSON-файлы коллекции и окружения.
    \item Такой подход гарантирует, что система протестирована в условиях, максимально приближенных к продакшену. Проверяется не только логика отдельных API, но и сетевое взаимодействие через Envoy, а также асинхронный обмен сообщениями через NATS. Это позволяет убедиться в корректной работе всего цикла запросов и ответов между сервисами.
\end{itemize}

\subsection*{Выводы по главе}
\addcontentsline{toc}{subsection}{Выводы по главе}
Таким образом, в этой главе продемонстрирована целостная реализация серверной части приложения «Путешествия по России» — от выбора и обоснования единого технологического стека (Go 1.24, gRPC, PostgreSQL, Redis, NATS JetStream, MinIO) до детального описания микросервисов и DevOps потока, подтверждённого комплексным тестированием. Все сервисы построены на принципах чистой архитектуры, чётко разделяют доменные области (Auth, Profile, Content, Activity, FileStorage, Notifications) и общаются строго типизированными gRPC контрактами, а асинхронные задачи надёжно обрабатываются через JetStream с гарантией хотя бы однократной доставки. Контейнеризация и единая система Helm чартов обеспечивают воспроизводимое развёртывание всей платформы в локальном Kubernetes кластере на выделенной виртуальной машине (4 vCPU 3,3 ГГц, 8 ГБ RAM, 80 ГБ NVMe), где при штатной работе потребление памяти не превышает 1,5 ГБ. Автоматизированные CI/CD конвейеры GitHub Actions формируют минимальные Docker образы, публикуют их вместе с чартами в GHCR и запускают Helm upgrade, гарантируя непрерывную доставку и миграцию схем БД без простоев; структурированное логирование на базе zap и ротация lumberjack предусматривают последующее подключение централизованной системы сбора логов. Юнит тесты (testing + testify) подтверждают корректность ключевой бизнес логики в изоляции, а интеграционные сценарии Postman/Newman, исполняемые в dev кластере через helm test, удостоверяют согласованность микросервисного взаимодействия и работу критических путей через Envoy и JetStream в среде, максимально приближённой к production. Совокупность изложенных решений демонстрирует, что реализованная архитектура не только удовлетворяет функциональным требованиям, но и обладает заложенными во второй главе нефункциональными характеристиками — масштабируемостью, отказоустойчивостью, безопасностью и готовностью к дальнейшему расширению.

Такая структура проекта обеспечивает хорошую организацию кода и четкое разделение ответственности между компонентами. Такая структура каталогов соответствует принципам clean architecture: слой инфраструктуры отделен от бизнес-логики, а конфигурация выделена в отдельный модуль для упрощения управления настройками приложения.

В данной главе представлена подробная реализация серверной части мобильного приложения «Путешествия по России» на языке Go с использованием микросервисной архитектуры. Разработанная система включает 6 основных микросервисов: Auth для аутентификации, Profile для управления профилями, Content для работы с маршрутами, Activity для социальных взаимодействий, FileStorage для работы с файлами и Notifications для отправки уведомлений.
